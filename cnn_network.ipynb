{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f9d309-cb87-4471-9a20-9e8c72bff8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.utils.data.sampler import  SubsetRandomSampler  #for validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd0c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that MPS is available, if not, check if CUDA is available, if not, use CPU\n",
    "if not torch.backends.mps.is_available():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:1\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adc00e-5c98-4f04-9041-72bb82fd3f54",
   "metadata": {},
   "source": [
    "(i)knowledge-oblivious–the attacker shall have no knowledge of the target\n",
    "model’s parameters/structures, nor the original training datasets, \n",
    "\n",
    "(ii) cleanlabel–the attacker shall not be able to control the labeling process, and\n",
    "\n",
    "(iii)clean-test–test-time instances shall not be required to be modified using added\n",
    "adversarial perturbations for attacking effectiveness\n",
    "\n",
    "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720137.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1210b8-cf64-4f9c-8bbf-1bd2ef83378a",
   "metadata": {},
   "source": [
    "Make sure that you do not reuse the data for training, testing and the attack/defence. - prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d388123-cafd-420b-a1e2-ab9bfc8eed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(0.2859, 0.3530)\n",
    "                               ])\n",
    "\n",
    "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
    "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
    "\n",
    "#Preparing for validaion test\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "#to get 20% of the train set\n",
    "split = int(np.floor(0.2 * len(trainset)))\n",
    "train_sample = SubsetRandomSampler(indices[:split])\n",
    "valid_sample = SubsetRandomSampler(indices[split:])\n",
    "\n",
    "#Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=64)\n",
    "validloader = torch.utils.data.DataLoader(trainset, sampler=valid_sample, batch_size=64)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4492473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "def get_item(preds, labels):\n",
    "    \"\"\"function that returns the accuracy of our architecture\"\"\"\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "@torch.no_grad() # turn off gradients during inference for memory effieciency\n",
    "def get_all_preds(network, dataloader):\n",
    "    \"\"\"function to return the number of correct predictions across data set\"\"\"\n",
    "    all_preds = torch.tensor([])\n",
    "    model = network\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        preds = model(images) # get preds\n",
    "        all_preds = torch.cat((all_preds, preds), dim=0) # join along existing axis\n",
    "        \n",
    "    return all_preds\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels=6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features= 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features= 60, out_features = 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        \n",
    "        # hidden layer 1\n",
    "        tensor = self.conv1(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.max_pool2d(tensor, kernel_size = 2, stride= 2)\n",
    "        \n",
    "        # hidden layer 2\n",
    "        \n",
    "        tensor = self.conv2(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.max_pool2d(tensor, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        #hidden layer 3\n",
    "        \n",
    "        tensor = tensor.reshape(-1, 12 * 4* 4)\n",
    "        tensor = self.fc1(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        \n",
    "        #hidden layer 4\n",
    "        \n",
    "        tensor = self.fc2(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        \n",
    "        #output layer\n",
    "        \n",
    "        tensor = self.out(tensor)\n",
    "        \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8225fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionCNN(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = FashionCNN().to(device)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = 0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(cnn_model) # print model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1675d16f-c5c9-42bc-9f0e-8354bb13e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10.. Training loss: 0.813.. Validation loss: 0.576.. Validation Accuracy: 77.981\n",
      "Epoch: 2/10.. Training loss: 0.516.. Validation loss: 0.486.. Validation Accuracy: 81.310\n",
      "Epoch: 3/10.. Training loss: 0.445.. Validation loss: 0.433.. Validation Accuracy: 83.983\n",
      "Epoch: 4/10.. Training loss: 0.392.. Validation loss: 0.456.. Validation Accuracy: 83.331\n",
      "Epoch: 5/10.. Training loss: 0.362.. Validation loss: 0.422.. Validation Accuracy: 84.785\n",
      "Epoch: 6/10.. Training loss: 0.345.. Validation loss: 0.393.. Validation Accuracy: 86.396\n",
      "Epoch: 7/10.. Training loss: 0.313.. Validation loss: 0.396.. Validation Accuracy: 85.685\n",
      "Epoch: 8/10.. Training loss: 0.303.. Validation loss: 0.397.. Validation Accuracy: 86.267\n",
      "Epoch: 9/10.. Training loss: 0.288.. Validation loss: 0.425.. Validation Accuracy: 85.846\n",
      "Epoch: 10/10.. Training loss: 0.272.. Validation loss: 0.403.. Validation Accuracy: 85.571\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "epochs = 10\n",
    "\n",
    "# Lists for knowing classwise accuracy\n",
    "predictions_list, labels_list = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "  running_loss = 0\n",
    "  for images, labels in trainloader:\n",
    "    # Flatten Fashion-MNIST images into a 784 long vector\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Training pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = cnn_model.forward(images)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "  else:\n",
    "    valid_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    # Turn off gradients for validation, saves memory and computation\n",
    "    with torch.no_grad():\n",
    "      # Set the model to evaluation mode\n",
    "      cnn_model.eval()\n",
    "      \n",
    "      # Validation pass\n",
    "      for images, labels in validloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        log_ps = cnn_model(images)\n",
    "        valid_loss += criterion(log_ps, labels)\n",
    "        \n",
    "        predictions = torch.max(log_ps, 1)[1].to(device)\n",
    "        predictions_list.append(predictions)\n",
    "        correct += (predictions == labels).sum()\n",
    "        total += len(labels)\n",
    "    \n",
    "    accuracy = correct * 100 / total\n",
    "    train_losses.append(running_loss/len(trainloader))\n",
    "    valid_losses.append(valid_loss/len(validloader))\n",
    "    cnn_model.train()\n",
    "    \n",
    "    \n",
    "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
    "          \"Validation loss: {:.3f}..\".format(valid_loss/len(validloader)),\n",
    "          \"Validation Accuracy: {:.3f}\".format(accuracy))\n",
    "    torch.save(cnn_model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f0fb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5790974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/Top: 81.20%\n",
      "Accuracy of Trouser: 91.60%\n",
      "Accuracy of Pullover: 72.50%\n",
      "Accuracy of Dress: 90.90%\n",
      "Accuracy of Coat: 86.50%\n",
      "Accuracy of Sandal: 94.80%\n",
      "Accuracy of Shirt: 47.80%\n",
      "Accuracy of Sneaker: 97.20%\n",
      "Accuracy of Bag: 96.10%\n",
      "Accuracy of Ankle Boot: 91.90%\n"
     ]
    }
   ],
   "source": [
    "class_correct = [0. for _ in range(10)]\n",
    "total_correct = [0. for _ in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        test = Variable(images)\n",
    "        outputs = cnn_model(test)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            total_correct[label] += 1\n",
    "        \n",
    "for i in range(10):\n",
    "    print(\"Accuracy of {}: {:.2f}%\".format(output_label(i), class_correct[i] * 100 / total_correct[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d5ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2eded2f99aeb90719e888b4c5be52669e449fec0a21fe7ddff5c2d5ec2cc4c84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
